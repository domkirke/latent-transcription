## Cross-modal variational inference for musical transcription and generation

This repository hosts the code for the article "Cross-modal variational inference for musical transcription and generation" submitted at IJCNN 2019. This code allows the training, the analysis, and various generation methods of the models presented in the article. It is divided in four main scripts :  

* `lt_train.py` allows model training
* `lt_analyze.py` performs the evaluation of the selected model 
* `lt_midi.py` performs the model evaluation over the test dataset
* `lt_play.py` allows various methods of generation using the selected model : midi import, audio import, and supervised / free navigation.

This code uses the marvelous code of Thomas Grill, gently put on the internet at [the following address](https://github.com/grrrr/nsgt)

### Model training :package:
`lt_train.py` is the python script for MIDI training. It requires the training dataset, that you may find [here](http://www.wolfgang.wang/lt_set.zip). Possible instruments are : `Flute`, `Piano`, `Alto-Sax`, `Trumpet-C`, `Violin`. Training arguments for this script are : 

```
  -h, --help            show this help message and exit
  
  Dataset options :
  
  --dbroot DBROOT       root path of the database (given .npy file)
  --savedir SAVEDIR     output directory
  --frames [FRAMES [FRAMES ...]]
                        frames taken in each sound file (empty for all file,
                        chunk id or chunk range)
                        
  Architecture options:                   
  --dims DIMS [DIMS ...]
                        number of latent dimensions
  --hidden_dims_1 HIDDEN_DIMS_1 [HIDDEN_DIMS_1 ...]
                        latent layerwise hidden dimensions for audio vae
  --hidden_num_1 HIDDEN_NUM_1 [HIDDEN_NUM_1 ...]
                        latent layerwise number of hidden layers for audio vae
  --hidden_num_2 HIDDEN_NUM_2 [HIDDEN_NUM_2 ...]
                        latent layerwise number of hidden layers for symbolic
                        vae
  --hidden_dims_2 HIDDEN_DIMS_2 [HIDDEN_DIMS_2 ...]
                        latent layerwise hidden dimensions for symbolic vae
  --labels [LABELS [LABELS ...]]
                        name of conditioning labels (octave, pitch, dynamics)
  --instruments INSTRUMENTS [INSTRUMENTS ...]
                        name of instruments (Flute, Piano, Trumpet-C, Alto-Sax, Violin)
  --label_type {binary,categorical}
                        label conditioning distribution
  --regularization_type {kld,l2}
                        latent regularization type between both latent spaces
  --random_mode {constant,bernoulli}
                        random weighing of each source in the mixtures
  --zero_extra_class ZERO_EXTRA_CLASS
                        has an extra zero class when source is silent
                        (recommanded with bernoulli random_mode)
                        
  Training options: 
  --epochs EPOCHS       nuber of training epochs
  --save_epochs SAVE_EPOCHS
                        saving epochs
  --plot_epochs PLOT_EPOCHS
                        plotting epochs
  --name NAME           name of current training
  --cuda CUDA           cuda id (-1 for cpu)
  --load LOAD
  --load_epoch LOAD_EPOCH
  
  Loss options : 
  --beta_1 BETA_1       beta regularization for signal vae
  --beta_2 BETA_2       beta regularization for symbol vae
  --cross_1 CROSS_1     cross-regularization between z_signal and z_symbol
  --cross_2 CROSS_2     cross-regularization between z_symbol and z_signal
  --adversarial ADVERSARIAL
                        set to 1 for adversarial reinfocement.
  --adv_dim ADV_DIM     hidden capacity for adversarial network
  --adv_num ADV_NUM     number of hidden networks for adversarial network
  --adv_lr ADV_LR       learning rate for adversarial network
  ```
The script will create a folder named `${name}+__${instruments}` at the specified output location (default : `saves/`). This folder will contain three folders : `vae_1`, containing the signal VAE,  `vae_2`, containing the symbol VAE, and `figures`, contaning the figures generated during the training.

**Examples**
```
python3 lt_train.py --dbroot ${DBROOT} --dims 32 --instruments Piano 
```

```
python3 lt_train.py --dbroot ${DBROOT} --dims 64 --instruments Piano Violin --hidden_dims_1 5000 --hidden_dims_2 1500 --name scan_multi_adversarial --adversarial 1
```
```
python3 lt_train.py --dbroot ${DBROOT} --dims 64 --instruments Trumpet-C --name scan_multi_nosignalreg --beta_1 0 --beta_2 2
```

### Model analysis
`lt_analyze.py` takes a list of models (specified by the main folder of the model , generated by the script `lt_train.py`), outputs the list of metrics discussed in the article (log-likelihood, Itakura-Saito Divergence, confusion matrix, classification ratio) and outputs some audio generations of random audio chunks picked in the dataset. It prints the result in the console, so it is advised to follow the command by ` > output_file` to register the results in a text file. The arguments for this script are
```
-h, --help            show this help message and exit
  -d DBROOT, --dbroot DBROOT
                        dataset path
  -m MODELS [MODELS ...], --models MODELS [MODELS ...]
                        model to load
  -c CUDA, --cuda CUDA  cuda device (leave -1 for GPU)
  -o OUTPUT, --output OUTPUT
                        results output
  -n NB_PASSES, --nb_passes NB_PASSES
                        number of passes for loss computation (useful in case
                        of random mixtures
  --classifier_epochs CLASSIFIER_EPOCHS
                        number of passes for loss computation
  --classifier_bs CLASSIFIER_BS
                        number of passes for loss computation
  --make_losses MAKE_LOSSES
                        compute losses
  --make_figures MAKE_FIGURES
                        make figures
  --make_classifier MAKE_CLASSIFIER
                        train baseline classifier
  --evaluate_classifier EVALUATE_CLASSIFIER
                        evaluate baseline classifier
  --generate_audio GENERATE_AUDIO
                        generate audio_examples (nb of examples)
  --n_samples N_SAMPLES
                        number of signal to symbol samples
```

**Examples**

```
python3 lt_analyze.py -d ${DBROOT} -m ${MODEL} --make_classifier 1 --evaluate_classifier 1 --classifier_epochs 1000
```

### Model MIDI import 
The script `lt_midi.py` generates the cross-dataset results obtained with the flute dataset built by Cantos & al., [available here](https://zenodo.org/record/1408985). The arguments for this script are 
```
-h, --help            show this help message and exit
  --models MODELS       List of models to evaluate
  --output OUTPUT       results output location
  --dataroot DATAROOT   audio file location
  --midiroot MIDIROOT   midi file location
  --cuda CUDA           cuda device id (-1 for cpu)
  --plot PLOT           generate plots
  --losses LOSSES       generate losses
  --resynthesize RESYNTHESIZE
                        synthesize sound samples
  --classifier CLASSIFIER
                        evaluate classifier (needs the test_classifier.t7 file
                        generated by lt_analyze.py)
```

```
python3 lt_midi.py --models ${MODEL} --dataroot ${DATASET_PATH}/flute-audio-labelled-database-AMT/Recordings --midiroot ${DATASET_PATH}/flute-audio-labelled-database-AMT/Aligned_files/mid_files
```

### Model AUDIO generation
The script `lt_play.py` takes trained models and generate sounds / labels according to different modes : 
* `info` just outputs information about the loaded model and its respective losses (if computed)
* `midi` loads midi files, extract the conditioning sequence and gives it to the model to generate the corresponding sound sequence.
* `audio` loads an audio file, outputs its audio reconstruction and the symbolic information transferred by the model.
* `sequence` takes an input sequence of labels, and output the corresponding sounds
* `morphing` also takes an input sequence of labels, but outputs the latent interpolation between corresponding latent distributions
* `trajectory` generates a latent trajectory, and outputs the corresponding audio sequence. 



